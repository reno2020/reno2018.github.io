
<html>
<body>

<h1>2018-08</h1>

<p>My first paragraph.</p>


https://www.cnblogs.com/kevingrace/  </br>
https://blog.csdn.net/chengqiuming?t=1  </br>
http://www.itmuch.com/ </br>
http://cxytiandi.com/article  </br>
https://blog.csdn.net/pucao_cug/article/category/6914103  </br>
http://www.cnblogs.com/ksWorld/category/983948.html  </br>
https://www.cnblogs.com/ilinuxer/category/752875.html  </br>
https://www.cnblogs.com/qingyunzong/p/8807252.html  </br>
http://mirrors.hust.edu.cn/apache/spark/spark-2.1.3/  </br>
http://mirror.bit.edu.cn/apache/sqoop/1.4.7/  </br>
http://mirror.bit.edu.cn/apache/hbase/stable/  </br>
https://www.scala-lang.org/download/ </br>





----------------------------------------------------------------------------</br>


https://www.cnblogs.com/qingyunzong/p/8807252.html  </br>
https://blog.csdn.net/pucao_cug/article/category/6914103  </br>
https://www.cnblogs.com/ilinuxer/category/752875.html  </br>

http://mirrors.hust.edu.cn/apache/spark/spark-2.1.3/  </br>
http://mirror.bit.edu.cn/apache/sqoop/1.4.7/  </br>
http://mirror.bit.edu.cn/apache/hbase/stable/  </br>
http://mirror.bit.edu.cn/apache/zookeeper/stable/  </br>
https://www.scala-lang.org/download/  </br>


http://10.9.14.110:8080/  </br>
http://10.9.14.110:16030/rs-status?filter=operation#regionMemstoreStats  </br>
http://10.9.14.110:50070/dfshealth.html#tab-overview  </br>

----------------------------------------------------------------------------</br>

允许root用户在任何地方进行远程登录，并具有所有库任何操作权限，

具体操作如下：

在本机先使用root用户登录mysql： mysql -u root -p"youpassword" 进行授权操作：

mysql>GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'youpassword' WITH GRANT OPTION;

重载授权表：

FLUSH PRIVILEGES;

退出mysql数据库：

exit

----------------------------------------------------------------------------</br>


 sqoop   list-tables  --username  root   --password  'root'     --connect     jdbc:mysql://127.0.0.1:3306/hello?characterEncoding=UTF-8




sqoop   create-hive-table    --connect    jdbc:mysql://127.0.0.1:3306/hello?characterEncoding=UTF-8    --table  test     --username   root   -password 'root'     --hive-database  db_hive_edu



sqoop    import     --connect     jdbc:mysql://127.0.0.1:3306/hello?characterEncoding=UTF-8&useSSL=false        --table    place    --username   root  -password  'root'    --fields-terminated-by   ','    --hive-import      --hive-database  db_hive_edu     -m  1





将mysql中的数据导入到hive中报错：

ERROR tool.ImportTool:Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf


这是因为sqoop需要一个hive的包，将hive/lib中的hive-common-2.3.3.jar拷贝到sqoop的lib目录中

重新执行试试。




hive启动出现权限错误 /tmp/hive on HDFS should be writable.

错误分析。

从The root scratch dir: /tmp/hive on HDFS should be writable. Currentpermissions are: rwxr-xr-x可以看出。系统要求在hdfs上用户应该具备写权限，而从报错可以看出只有所有者具有写权限，所有组合其他用户不具备。

错误修改。

使用hadoop fs hadoop fs -chmod -R dir修改目录权限，如下

[root@hadoop22 ~]# hadoop fs -chmod -R 777 /tmp

使其他用户也具备本目录的写权限




java连接jdbc Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by defa
conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/jsp_db","root","123456");

这个地方改成conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/jsp_db?useUnicode=true&characterEncoding=utf-8&useSSL=false","root","123456");




./bin/spark-submit  --class  org.apache.spark.examples.SparkPi  --master local   examples/jars/spark-examples_2.11-2.1.3.jar















https://blog.csdn.net/wind520?t=1



----------------------------------------------------------------------------</br>

大数据

https://www.cnblogs.com/qingyunzong/p/8807252.html
https://blog.csdn.net/pucao_cug/article/category/6914103
https://www.cnblogs.com/ilinuxer/category/752875.html

http://mirrors.hust.edu.cn/apache/spark/spark-2.1.3/
http://mirror.bit.edu.cn/apache/sqoop/1.4.7/
http://mirror.bit.edu.cn/apache/hbase/stable/
http://mirror.bit.edu.cn/apache/zookeeper/stable/
https://www.scala-lang.org/download/


http://10.9.14.110:8080/
http://10.9.14.110:16030/rs-status?filter=operation#regionMemstoreStats
http://10.9.14.110:50070/dfshealth.html#tab-overview




----------------------------------------------------------------------------</br>


mysql->zookeeper->hadoop->hive->hbase->spark

service mysql start   service mysql stop   service mysql restart
mysql -u root -p



/usr/program/zookeeper-3.4.12/bin
./zkServer.sh start     ./zkServer.sh stop


/usr/program/hadoop-2.8.0/sbin
./start-all.sh    ./stop-all.sh


/usr/program/hive-2.3.3/bin
./hive

/usr/program/hbase-1.2.6.1/bin
./start-hbase.sh


/usr/program/spark-2.1.3-bin-hadoop2.7/sbin
./start-all.sh


/bin
./spark-submit  --class org.apache.spark.examples.SparkPi  --master spark://reno-server:7077  --executor-memory 1G  --total-executor-cores 1  /usr/program/spark-2.1.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.3.jar  100





----------------------------------------------------------------------------</br>


大数据学习步骤

http://www.cnblogs.com/zlslch/p/5448857.html

　注意：下面列出来的顺序只是个人建议，可以根据个人实际情况来调整顺序

linux基础和javase基础【包含mysql】
这些是基本功，刚开始也不可能学的很精通，最起码要对linux中的一些基本的命令混个脸熟，后面学习各种框架的时候都会用到，用多了就熟悉了。javase的话建议主要看面向对象，集合，io，多线程，以及jdbc操作即可。
zookeeper
zookeeper是很多大数据框架的基础，中文名称是动物园的意思，因为目前的大数据框架的图标很多都是动物的形状，所以zookeeper其实就是可以管理很多大数据框架的。针对这个框架，主要掌握如何搭建单节点和集群，以及掌握如何在zkcli客户端下对zookeeper的节点进行增删改查操作即可。
hadoop
目前企业中一般都是用hadoop2.x的版本了，所以就没有必要再去学hadoop1.x版本了，hadoop2.x主要包含三大块
hdfs 前期，主要学习hdfs的一些命令即可，上传，下载，删除，移动，查看等命令...
mapreduce 这个需要重点学习下，要理解mr的原理以及代码实现，虽然现在工作中真正写mr的代码次数很少了，但是原理还是要理解的。
yarn 前期了解即可，只需要知道yarn是一个资源调度平台，主要负责给任务分配资源即可，yarn不仅可以给mapreduce任务调度资源，还可以为spark任务调度资源...yarn是一个公共的资源调度平台，所有满足条件的框架都可以使用yarn来进行资源调度。
hive
hive是一个数据仓库，所有的数据都是存储在hdfs上的，具体【数据仓库和数据库】的区别大家可以去网上搜索一下，有很多介绍。其实如果对mysql的使用比较熟悉的话，使用hive也就简单很多了，使用hive主要是写hql，hql是hive的sql语言，非常类似于mysql数据库的sql，后续学习hive的时候主要理解一些hive的语法特性即可。其实hive在执行hql，底层在执行的时候还是执行的mapredce程序。
注意：其实hive本身是很强大的，数据仓库的设计在工作中也是很重要的，但是前期学习的时候，主要先学会如何使用就好了。后期可以好好研究一下hive。
hbase
hbase是一个nosql 数据库，是一个key-value类型的数据库，底层的数据存储在hdfs上。在学习hbase的时候主要掌握 row-key的设计，以及列簇的设计。要注意一个特点就是，hbase基于rowkey查询效率很快，可以达到秒级查询，但是基于列簇中的列进行查询，特别是组合查询的时候，如果数据量很大的话，查询性能会很差。
redis
redis也是一个nosql 数据库和key-value类型的数据库，但是这个数据库是纯基于内存的，也就是redis数据库中的数据都是存储在内存中的，所以它的一个特点就是适用于快速读写的应用场景，读写可以达到10W次/秒，但是不适合存储海量数据，毕竟机器的内存是有限的，当然，redis也支持集群，也可以存储大量数据。在学习redis的时候主要掌握string，list，set，sortedset，hashmap这几种数据类型的区别以及使用，还有pipeline管道，这个在批量入库数据的时候是非常有用的，以及transaction事务功能。
flume
flume是一个日志采集工具，这个还是比较常用的，最常见的就是采集应用产生的日志文件中的数据。一般有两个流程，一个是flume采集数据存储到kafka中，为了后面使用storm或者sparkstreaming进行实时处理。另一个流程是flume采集的数据落盘到hdfs上，为了后期使用hadoop或者spark进行离线处理。在学习flume的时候其实主要就是学会看flume官网的文档，学习各种组建的配置参数，因为使用flume就是写各种的配置。
kafka
kafka 是一个消息队列，在工作中常用于实时处理的场景中，作为一个中间缓冲层，例如，flume->kafka->storm/sparkstreaming。学习kafka主要掌握topic，partition，replicate等的概念和原理。
storm
storm是一个实时计算框架，和hadoop的区别就是，hadoop是对离线的海量数据进行处理，而storm是对实时新增的每一条数据进行处理，是一条一条的处理，可以保证数据处理的时效性。学习storm主要学习topology的编写，storm并行度的调整，以及storm如何整合kafka实时消费数据。
spark
spark 现在发展的也很不错，也发展成了一个生态圈，spark里面包含很多技术，spark core，spark steaming，spark mlib，spark graphx。
spark生态圈里面包含的有离线处理spark core，和实时处理spark streaming，在这里需要注意一下，storm和spark streaming ，两个都是实时处理框架，但是主要区别是：storm是真正的一条一条的处理，而spark streaming 是一批一批的处理。
spark中包含很多框架，在刚开始学习的时候主要学习spark core和spark streaming即可。这个一般搞大数据的都会用到。spark mlib和spark graphx 可以等后期工作需要或者有时间了在研究即可。
elasticsearch
elasticsearch是一个适合海量数据实时查询的全文搜索引擎，支持分布式集群，其实底层是基于lucene的。在查询的时候支持快速模糊查询，求count，distinct，sum，avg等操作，但是不支持join操作。elasticsearch目前也有一个生态圈，elk(elasticsearch logstash kibana)是一个典型的日志收集，存储，快速查询出图表的一整套解决方案。在学习elasticsearch的时候，前期主要学习如何使用es进行增删改查，es中的index，type，document的概念，以及es中的mapping的设计。
 

　　目前暂且列出来这么多吧，大数据生态圈目前还有很多比较好的技术框架，这个就需要等大家以后工作之后再去扩展了。

　　其实上面列出来的这十几个框架，在学习的时候，要专门挑一两个着重研究一下，最好针对，底层原理，优化，源码等部分有所涉猎，这么的话可以在面试过程中脱颖而出。不要想着把每一个框架都搞精通，目前是不现实的，其实就算是在工作中也不会每一个框架都会用的很深。

　　如果能过对上面的框架都大致会使用，并且对某一两个框架研究的比较深的话，其实想去找一份满意的大数据工作也就水到渠成了。


----------------------------------------------------------------------------</br>
----------------------------------------------------------------------------</br>
----------------------------------------------------------------------------</br>

</body>
</html>
